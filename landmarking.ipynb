{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "\n",
    "from preamble import *\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "import math\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy import arange\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import Imputer, LabelEncoder, MinMaxScaler, StandardScaler, Normalizer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import cross_val_score  \n",
    "\n",
    "from pymongo import MongoClient\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following Landmarking meta-features are calculated: (Matthias Reif et al. 2012, Abdelmessih et al. 2010)\n",
    "\n",
    "The accuracy values of the following simple learners are used: Naive Bayes, Linear Discriminant Analysis, One-Nearest Neighbor, Decision Node, Random Node.\n",
    "\n",
    "- **Naive Bayes Learner** is a probabilistic classifier, based on Bayesâ€™ Theorem:\n",
    "$$ p(X|Y ) = \\frac{p(Y|X) \\cdot p(X)}{p(Y )} $$\n",
    "\n",
    "    where p(X) is the prior probability and p(XjY ) is the posterior probability. It is called naive, because it\n",
    "    assumes independence of all attributes to each other.\n",
    "- **Linear Discriminant Learner** is a type of discriminant analysis, which is understood as the grouping and separation of categories according to specific features. Linear discriminant is basically finding a linear combination of features that separates the classes best. The resulting separation model is a line, a plane, or a hyperplane, depending on the number of features combined. \n",
    "\n",
    "- **One Nearest Neighbor Learner** is a classifier based on instance-based learning. A test point is assigned to the class of the nearest point within the training set. \n",
    "\n",
    "- **Decision Node Learner** is a classifier based on the information gain of attributes. The information gain indicates how informative an attribute is with respect to the classification task using its entropy. The higher the variability of the attribute values, the higher its information gain. This learner selects the attribute with the highest information gain. Then, it creates a single node decision tree consisting of the chosen attribute as a split node. \n",
    "\n",
    "- **Randomly Chosen Node Learner** is a classifier that results also in a single decision node, based on a randomly chosen attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline(dataset, X, y, estimator):\n",
    "    pipe = Pipeline([('Imputer', preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0)),\n",
    "#                     (\"scaler\", MinMaxScaler()),\n",
    "#                     ('OneHotEncoder', preprocessing.OneHotEncoder(sparse=False, handle_unknown='ignore')),\n",
    "                     ('classifiers', estimator)])\n",
    "    score = np.mean(cross_val_score(pipe, X, y, cv=10, scoring='roc_auc', n_jobs=-1))\n",
    "    scores.append(score)\n",
    "    score_tmp.append(score)\n",
    "    classifier.append(str(estimator.__class__.__name__))\n",
    "\n",
    "scores = []\n",
    "classifier = []\n",
    "datasets = np.genfromtxt('datasetID2.csv', delimiter=',', dtype=int)\n",
    "db = connet_mongoclient('109.238.10.185')\n",
    "\n",
    "for dataset in datasets:\n",
    "    score_tmp = []\n",
    "    data = oml.datasets.get_dataset(dataset) \n",
    "    X, y = data.get_data(target=data.default_target_attribute)\n",
    "    if len(set(y)) <= 2:\n",
    "        pipeline(dataset, X, y, KNeighborsClassifier(n_neighbors = 1)) # One-nearest neighbor\n",
    "        pipeline(dataset, X, y, LinearDiscriminantAnalysis(solver='lsqr', shrinkage='auto')) # Linear Discriminant Analysis\n",
    "        pipeline(dataset, X, y, GaussianNB()) # Gaussian Naive Bayes\n",
    "        pipeline(dataset, X, y, DecisionTreeClassifier(criterion='entropy', splitter='best', \n",
    "                                    max_depth=1, random_state=0)) # Decision Node Learner\n",
    "        pipeline(dataset, X, y, DecisionTreeClassifier(criterion='entropy', splitter='random',\n",
    "                                    max_depth=1, random_state=0)) # Randomly Chosen Node Learner\n",
    "        print(dataset, score_tmp)\n",
    "        db.landmarkers.insert_one({'dataset': str(dataset),\n",
    "                                   'score': score_tmp})\n",
    "    else: \n",
    "        multiclass.append(dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
